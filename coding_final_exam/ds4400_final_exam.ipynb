{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483eb75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396526c",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86eb9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = genfromtxt('life_expectancy_X.csv', delimiter=',')\n",
    "A_test = genfromtxt('life_expectancy_X_test.csv', delimiter=',')\n",
    "\n",
    "y = genfromtxt('life_expectancy_y.csv', delimiter=',')\n",
    "y_test = genfromtxt('life_expectancy_y_test.csv', delimiter=',')\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "A = scaler.fit_transform(A)\n",
    "A_test = scaler.transform(A_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e844796",
   "metadata": {},
   "source": [
    "### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca66412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_prime(w, phi, y, l1_lambda):\n",
    "    return (1/len(phi))*(phi.T @ (phi@w - y)) + l1_lambda * np.sign(w)\n",
    "\n",
    "def gradient_descent(phi, y, w, eta, num_iterations, l1_lambda):\n",
    "\tfor i in range(num_iterations):\n",
    "\t\tw = w - eta * f_prime(w, phi, y, l1_lambda)  \n",
    "            \n",
    "\treturn w\n",
    "\n",
    "def mse_func(phi, w, y):\n",
    "\treturn np.mean((phi@w - y)**2)\n",
    "\n",
    "def sorted_features_importance(feature_names_list, w, lin_reg=False):\n",
    "\tif lin_reg == True:\n",
    "\t\tsorted_w = w[:-1]\n",
    "\telse:\n",
    "\t\tsorted_w = w\n",
    "\n",
    "\tsorted_indices = np.argsort(-np.abs(sorted_w))\n",
    "\tsorted_features = [feature_names_list[i] for i in sorted_indices]\n",
    "\tsorted_w = w[sorted_indices]\n",
    "\n",
    "\tfor feature, weight in zip(sorted_features, sorted_w):\n",
    "\t\tprint(f\"{feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c29b8149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.005947909788639809\n"
     ]
    }
   ],
   "source": [
    "phi = np.hstack((np.ones((A.shape[0], 1)), A))\n",
    "w0 = np.zeros(phi.shape[1])\n",
    "eta = 0.02\n",
    "l1_lambda = 0.01\n",
    "\n",
    "w = gradient_descent(phi, y, w0, eta, 10000, l1_lambda)\n",
    "\n",
    "phi_test = np.hstack((np.ones((A_test.shape[0], 1)), A_test))\n",
    "\n",
    "print(f'MSE: {mse_func(phi_test, w, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc4b44c",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577e9cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise amount: 59.8926\n",
      "Amount of supportive relationships: 29.9769\n",
      "Number of siblings: 14.9417\n",
      "Height: -9.8219\n",
      "Attractiveness: 0.0002\n",
      "Alcohol / Drugs / Smoking consumption: 0.0001\n",
      "work ethics: -0.0000\n"
     ]
    }
   ],
   "source": [
    "feature_names = [\n",
    "    \"Exercise amount\",\n",
    "    \"Amount of supportive relationships\",\n",
    "    \"Number of siblings\",\n",
    "    \"Alcohol / Drugs / Smoking consumption\",\n",
    "    \"Height\",\n",
    "    \"Attractiveness\",\n",
    "    \"work ethics\"\n",
    "]\n",
    "\n",
    "sorted_features_importance(feature_names, w, lin_reg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab76f5",
   "metadata": {},
   "source": [
    "Biggest Positive Impact: Exercise Amount, Supportive Relationshops, Number of Siblings  \n",
    "Negative Impact: Height  \n",
    "Attractiveness, Alcohol/Drugs/Smoking, and Work Ethic all have a minimal impact on longetivty  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b35ef0b",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5866147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_func(n):\n",
    "      C = np.eye(n) - (1/n) * np.ones((n, n))\n",
    "      return C\n",
    "\n",
    "def pca_reduc_func(X, num_dimensions):\n",
    "    n = len(X)\n",
    "\n",
    "    C = center_func(n)\n",
    "    Q = X.T @ C @ X\n",
    "\n",
    "    [D, V] = np.linalg.eigh(Q)\n",
    "\n",
    "    v = V[:, -num_dimensions:]\n",
    "\n",
    "    X_hat = X @ v \n",
    "\n",
    "    return X_hat, v, D \n",
    "\n",
    "def f_prime(w, phi, y):\n",
    "    return (1/len(phi))*(phi.T @ (phi@w - y))\n",
    "\n",
    "def gradient_descent(phi, y, w, eta, num_iterations):\n",
    "    for i in range(num_iterations):\n",
    "        w = w - eta * f_prime(w, phi, y)  \n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91ccbf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 58.21643737770854\n"
     ]
    }
   ],
   "source": [
    "pca_A, pca_v, eig_vals = pca_reduc_func(A, 4)\n",
    "\n",
    "pca_phi = np.hstack((np.ones((pca_A.shape[0], 1)), pca_A))\n",
    "w0 = np.zeros(pca_phi.shape[1])\n",
    "eta = 0.05\n",
    "\n",
    "pca_w = gradient_descent(pca_phi, y, w0, eta, 20000)\n",
    "\n",
    "A_test_C = center_func(len(A_test))\n",
    "pca_A_test = A_test @ pca_v\n",
    "\n",
    "pca_phi_test = np.hstack((np.ones((pca_A_test.shape[0], 1)), pca_A_test))\n",
    "print(f'Test MSE: {mse_func(pca_phi_test, pca_w, y_test)}')\n",
    "# print(f'MSE: {mse_func(pca_phi, pca_w, y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12164d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09306744,  0.2454878 , -0.25985832,  0.05647923, -0.18328759,\n",
       "       -0.27576156, -0.02681717])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_contributions = np.mean(pca_v, axis=1)\n",
    "feature_contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14cb7e4",
   "metadata": {},
   "source": [
    "### 4)\n",
    "PCA probably is not a good idea for this problem as shown by the much higher MSE than regular LASSO Linear Regression. PCA tries to maximize variance but does not account for the labels. Therefore, features which have low variance but high importance for predicting longetivity will have their importance reduce when PCA is applied. For example, number of sibilings and also relationships probably have a low variance but are of high importance to predicting longetivity, so with PCA their importance would be reduced. Therefore, using linear regression is not a good predictor for this problem and should not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e654f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8508629239616721, 0.43643566943179135, 0.002459752414860447, -0.3122734519911499, -0.036254143816136813, -0.03482415007593405, 0.03136403248523949]\n"
     ]
    }
   ],
   "source": [
    "def cov_func(X, Y):\n",
    "    n = len(X)\n",
    "    x_mean = np.mean(X)\n",
    "    y_mean = np.mean(Y)\n",
    "\n",
    "    return (1/n) * sum((X[i] - x_mean)*(Y[i] - y_mean) for i in range(n))\n",
    "\n",
    "\n",
    "def corr_func(X, Y):\n",
    "    return (cov_func(X, Y))/(np.sqrt((cov_func(X, X))*(cov_func(Y, Y))))\n",
    "\n",
    "corr_list = []\n",
    "for xi in A.T:\n",
    "    corr = corr_func(xi, y)\n",
    "    corr_list.append(corr)\n",
    "\n",
    "print(corr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f80eaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise amount: 0.8509\n",
      "Amount of supportive relationships: 0.4364\n",
      "Alcohol / Drugs / Smoking consumption: -0.3123\n",
      "Height: -0.0363\n",
      "Attractiveness: -0.0348\n",
      "work ethics: 0.0314\n",
      "Number of siblings: 0.0025\n"
     ]
    }
   ],
   "source": [
    "# Check correlations with feature names\n",
    "feature_names = [\n",
    "    \"Exercise amount\",\n",
    "    \"Amount of supportive relationships\",\n",
    "    \"Number of siblings\",\n",
    "    \"Alcohol / Drugs / Smoking consumption\",\n",
    "    \"Height\",\n",
    "    \"Attractiveness\",\n",
    "    \"work ethics\"\n",
    "]\n",
    "\n",
    "sorted_features_importance(feature_names, np.array(corr_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4742d",
   "metadata": {},
   "source": [
    "Exercise amount and supportive relationships have a significant positive correlation with longetivty.  \n",
    "Alcohol/drugs has a fairly siginificant negative correlation with longetivty.  \n",
    "Height, attractivness, work ethics, and number of siblings have a minimal correlation with longeivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8077f",
   "metadata": {},
   "source": [
    "### 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082d676",
   "metadata": {},
   "source": [
    "Both excericise amount and amount of supportive relationshops have a big postitive impact according to correlation and the LASSO weights. However, Alcohol/drugs consumption has a decently high negative correlation, while the LASSO weight is basically zero. Furthermore, according to the weights, number of siblings is the third most important feature, but it has a very low correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3096cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol / Drugs / Smoking consumption: 0.3101\n",
      "Attractiveness: 0.3002\n",
      "Number of siblings: 0.2965\n",
      "Amount of supportive relationships: 0.2455\n",
      "work ethics: 0.2313\n",
      "Height: 0.2184\n",
      "Exercise amount: 0.1348\n"
     ]
    }
   ],
   "source": [
    "# Use eigenvalues to find most impactful features for PCA\n",
    "eig_features = np.mean(np.abs(pca_v), axis=1)\n",
    "\n",
    "sorted_features_importance(feature_names, eig_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db0ee5",
   "metadata": {},
   "source": [
    "For PCA, the order of impactfulness is: alcohol consumption, attractiveness, number of siblings, amount of relationships, work ethic, height, and finally exercise amount. This is the opposite of correlation and LASSO weights, as exercise is the most impactful feature according to those metrics. Attractiveness is also low for both of those metrics, while attractiveness has the second biggest impact on PCA. Alcohol consumption has a somewhat high negative correlation, and is also the most impactful PCA feature. Overall, the correlation table matches the LASSO weights more than the PCA feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4f539",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe587cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = genfromtxt('gpa_prediction_X.csv', delimiter=',')\n",
    "A_test = genfromtxt('gpa_prediction_X_test.csv', delimiter=',')\n",
    "\n",
    "y = genfromtxt('gpa_prediction_y.csv', delimiter=',')\n",
    "y_test = genfromtxt('gpa_prediction_y_test.csv', delimiter=',')\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "A = scaler.fit_transform(A)\n",
    "A_test = scaler.transform(A_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a186b",
   "metadata": {},
   "source": [
    "### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb887cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_func(predictions, y):\n",
    "    return np.mean(predictions == y)\n",
    "\n",
    "# Logistic Regression\n",
    "def f_prime(w, phi, y, l1_lambda):\n",
    "    return 1/len(phi) * phi.T @ (sigmoid(phi, w) - y) + (l1_lambda * np.sign(w))\n",
    "\n",
    "def sigmoid(phi, w):\n",
    "    return 1/(1 + np.exp(-phi @ w))\n",
    "\n",
    "def gradient_descent(phi, y, w, eta, num_iterations, l1_lambda=0):\n",
    "\tfor i in range(num_iterations):\n",
    "\t\tw = w - eta * f_prime(w, phi, y, l1_lambda)  \n",
    "            \n",
    "\treturn w\n",
    "\n",
    "def log_reg_pred(phi, w):\n",
    "      return np.where(sigmoid(phi, w) >= 0.5, 1, 0)\n",
    "\n",
    "\n",
    "# def run_lambda_values(w0, X, y, lambdas, eta):\n",
    "#     best_acc = -np.inf  # store initial accuracy as high value\n",
    "#     best_lambda = None\n",
    "#     best_w = w0\n",
    "\n",
    "\n",
    "#     for l1_lambda in lambdas:\n",
    "        # w = gradient_descent(A, y, w0, eta, 10000, l1_lambda)\n",
    "\n",
    "        # predictions = log_reg_pred(A, w)\n",
    "        # acc = accuracy_func(predictions, y)\n",
    "\n",
    "    #     if acc > best_acc:  # if new accuracy is better, store value\n",
    "    #         best_acc = acc\n",
    "    #         best_lambda = l1_lambda\n",
    "    #         best_w = w\n",
    "\n",
    "    # return best_w, best_lambda, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b27f5a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.955\n"
     ]
    }
   ],
   "source": [
    "phi = np.hstack((np.ones((A.shape[0], 1)), A))\n",
    "\n",
    "w0 = np.zeros(phi.shape[1])\n",
    "eta = 0.002\n",
    "l1_lambda = 0.02\n",
    "\n",
    "w = gradient_descent(phi, y, w0, eta, 10000, l1_lambda)\n",
    "\n",
    "# Accuracy using test\n",
    "phi_test = np.hstack((np.ones((A_test.shape[0], 1)), A_test))\n",
    "predictions = log_reg_pred(phi_test, w)\n",
    "acc = accuracy_func(predictions, y_test)\n",
    "print(f'Test Accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded6a2f",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34801d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_features_importance(feature_names_list, w, lin_reg=False):\n",
    "\tif lin_reg == True:\n",
    "\t\tsorted_w = w[:-1]\n",
    "\telse:\n",
    "\t\tsorted_w = w\n",
    "\n",
    "\tsorted_indices = np.argsort(-np.abs(sorted_w))\n",
    "\tsorted_features = [feature_names_list[i] for i in sorted_indices]\n",
    "\tsorted_w = w[sorted_indices]\n",
    "\n",
    "\tfor feature, weight in zip(sorted_features, sorted_w):\n",
    "\t\tprint(f\"{feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4b902ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height: -1.1829\n",
      "Number of siblings: 0.1943\n",
      "Exercise amount: 0.0001\n",
      "Attractiveness: 0.0000\n",
      "work ethics: -0.0000\n",
      "Alcohol / Drugs / Smoking consumption: -0.0000\n",
      "Amount of supportive relationships: -0.0000\n"
     ]
    }
   ],
   "source": [
    "features = [\"Exercise amount\",\n",
    "\"Amount of supportive relationships\",\n",
    "\"Number of siblings\",\n",
    "\"Alcohol / Drugs / Smoking consumption\",\n",
    "\"Height\",\n",
    "\"Attractiveness\",\n",
    "\"work ethics\"]\n",
    "\n",
    "sorted_features_importance(features, w, lin_reg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acefb9f5",
   "metadata": {},
   "source": [
    "The number of siblings postiviely impacts your grade, while height has a huge negative impact. Supportive relationships, attractiveness, alcohol consumption, exercise amount, and work ethic all have minimal impact on your GPA according to the logistic regression weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987408a",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0ff1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_func(n):\n",
    "      C = np.eye(n) - (1/n) * np.ones((n, n))\n",
    "      return C\n",
    "\n",
    "def lda_q_func(X, y):\n",
    "      X0 = X[y == 0]  \n",
    "      X1 = X[y == 1]\n",
    "\n",
    "      c0 = np.reshape(np.mean(X0, axis=0), (X0.shape[1], 1))\n",
    "      c1 = np.reshape(np.mean(X1, axis=0), (X1.shape[1], 1))\n",
    "\n",
    "      u = c0 - c1\n",
    "      S1 = u @ (u.T)\n",
    "\n",
    "      C0 = center_func(len(X0))\n",
    "      C1 = center_func(len(X1))\n",
    "\n",
    "      S2 = (X0.T @ C0 @ X0) + (X1.T @ C1 @ X1)\n",
    "\n",
    "      Q = np.linalg.inv(S2) @ S1\n",
    "      return Q\n",
    "\n",
    "def lda_eig(X, y, num_dimensions):\n",
    "      Q = lda_q_func(X, y)\n",
    "\n",
    "      [D, V] = np.linalg.eigh(Q)\n",
    "\n",
    "      v = V[:, -num_dimensions:]\n",
    "\n",
    "      X_hat = X @ v \n",
    "\n",
    "      return X_hat, v, D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af6a3d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy using LDA: 0.975\n"
     ]
    }
   ],
   "source": [
    "lda_A, lda_v, lda_eig_vals = lda_eig(A, y, 3)\n",
    "\n",
    "lda_phi = np.hstack((np.ones((lda_A.shape[0], 1)), lda_A))\n",
    "w0 = np.zeros(lda_phi.shape[1])\n",
    "eta = 0.05\n",
    "\n",
    "lda_w = gradient_descent(lda_phi, y, w0, eta, 20000)\n",
    "\n",
    "# Test accuracy\n",
    "lda_A_test = A_test @ lda_v\n",
    "lda_phi_test = np.hstack((np.ones((lda_A_test.shape[0], 1)), lda_A_test))\n",
    "\n",
    "lda_predictions = log_reg_pred(lda_phi_test, lda_w)\n",
    "acc = accuracy_func(lda_predictions, y_test)\n",
    "print(f'Test Accuracy using LDA: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1466ba",
   "metadata": {},
   "source": [
    "### 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31e6f906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol / Drugs / Smoking consumption: -0.7128\n",
      "Amount of supportive relationships: 0.3844\n",
      "Attractiveness: -0.1825\n",
      "work ethics: 0.1543\n",
      "Height: 0.1333\n",
      "Number of siblings: 0.1111\n",
      "Exercise amount: 0.0183\n"
     ]
    }
   ],
   "source": [
    "lda_w_proj = lda_v @ lda_w[:-1]\n",
    "\n",
    "sorted_features_importance(features, lda_w_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1846a58",
   "metadata": {},
   "source": [
    "### 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c41396",
   "metadata": {},
   "source": [
    "The important factors in LDA are much different than the important ones for logistic regression. The only factors with any signficant weight according to the original logistic regression were Height, which had a negative impact and number of sibilings, which had a small positive impact. However, after LDA, Alcohol consumption had a big negative weight, then number of supportive relationships had a positive weight, both of which had no importance at all before LDA. Number of siblins and height both had a small weight for LDA, but not negligible. Exercise amount had a low impact for both logistic regression, and LDA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
