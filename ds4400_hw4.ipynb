{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance for a): 17.2\n"
     ]
    }
   ],
   "source": [
    "# 2)\n",
    "import numpy as np\n",
    "# a)\n",
    "sample = np.array([4, 7, 9, 11, 15])\n",
    "var = np.var(sample, ddof=1)  # use ddof of 1 because sample\n",
    "print(f'Variance for a): {var}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance for b): 68.26666666666668\n"
     ]
    }
   ],
   "source": [
    "# b)\n",
    "sample = np.array([18, 22, 25, 30, 35, 40])\n",
    "var = np.var(sample, ddof=1)  # use ddof of 1 because sample\n",
    "print(f'Variance for b): {var}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance for a): 8.0\n"
     ]
    }
   ],
   "source": [
    "# 4)\n",
    "\n",
    "# a)\n",
    "sample = np.array([12, 14, 16, 18, 20])\n",
    "var = np.var(sample, ddof=0)  # use ddof 0 for population\n",
    "print(f'Variance for a): {var}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance for b): 46.666666666666664\n"
     ]
    }
   ],
   "source": [
    "# b)\n",
    "sample = np.array([28, 32, 36, 40, 44, 48])\n",
    "var = np.var(sample, ddof=0)  # use ddof 0 for population\n",
    "print(f'Variance for b): {var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\n",
    "a): They key difference between empirical and population variance calculations is when calculation the empirical variance, the denominator used is n-1 while for the population variance the denominator used is n <br>\n",
    "b): An example of where you would want to use population variance over empirical variance is for example if you wanted to find the variance of the ages of kids in a specific class. However, if you wanted to find the variance in ages for an entire school, it would be better to take a sample of kids' ages and then use the empirical variance in your calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W after one steps of gradient descent: [[-13.        ]\n",
      " [ -5.11111111]\n",
      " [  7.        ]\n",
      " [  9.44444444]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "eta = 0.5\n",
    "\n",
    "X = np.array([[1, -2, 0, 1], [-2, -1, 1, 2], [1, 2, -1, 1]])\n",
    "w0 = np.array([[-1], [0], [1], [1]])\n",
    "Y = np.array([[2], [3], [-1]])\n",
    "\n",
    "n = X.shape[0]\n",
    "\n",
    "def f_prime(w, lambda_value):\n",
    "    return ((2/n)*(X.T.dot(X.dot(w)-Y)) + 2 * lambda_value * w)\n",
    "\n",
    "w = w0.copy()\n",
    "for i in range(2):\n",
    "    w = w - eta * f_prime(w, 2)\n",
    "\n",
    "print(f'W after one steps of gradient descent: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "X = genfromtxt('stock_prediction_data.csv', delimiter=',')\n",
    "\n",
    "y = genfromtxt('stock_price.csv', delimiter=',')\n",
    "y = np.reshape(y, (len(y),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=0.5)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create second degree polynomial feature map\n",
    "X_train = PolynomialFeatures(degree=2).fit_transform(X_train)\n",
    "X_val = PolynomialFeatures(degree=2).fit_transform(X_val)\n",
    "X_test = PolynomialFeatures(degree=2).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_prime(phi, w, y, n, lambda1):\n",
    "    return (2/n) * phi.T.dot(phi.dot(w) - y) + lambda1 * np.sign(w)\n",
    "\n",
    "def mse_function(y_pred, y):\n",
    "    return np.mean((y_pred- y)**2)\n",
    "\n",
    "def gradient_descent(phi, y, num_iterations, coef, lambda1):\n",
    "    n, m = phi.shape\n",
    "\n",
    "    w = np.random.rand(m, 1)  # Generate initial w\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        w -= coef * f_prime(phi, w, y, n, lambda1)\n",
    "\n",
    "    # MSE for final interation\n",
    "    final_mse = mse_function(phi.dot(w), y)\n",
    "\n",
    "    return w, final_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn mse for lasso: 6.607796083797113\n",
      "Gradient descent mse for lasso: 2.0322905782354983\n"
     ]
    }
   ],
   "source": [
    "# Find MSE for sklearn and gradient descent\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=1)\n",
    "lasso.fit(X_train, y_train.flatten())\n",
    "y_hat = lasso.predict(X_train)\n",
    "y_hat = y_hat.reshape(-1, 1)\n",
    "\n",
    "sklearn_mse = mse_function(y_hat, y_train)\n",
    "print(f'sklearn mse for lasso: {sklearn_mse}')\n",
    "\n",
    "w, mse = gradient_descent(X_train, y_train, 10000, 0.02, 1)\n",
    "print(f'Gradient descent mse for lasso: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda value: 0.016681005372000592 with mse: 0.0631533753696244\n"
     ]
    }
   ],
   "source": [
    "# identify best lambda value\n",
    "values = np.logspace(-4, 1, 10) \n",
    "\n",
    "best_value = values[0] # initialize best value\n",
    "best_mse = float('inf') # Store intiial mse as very high value\n",
    "\n",
    "for lambda_value in values:\n",
    "    # Get predicted w for lambda\n",
    "    w, mse = gradient_descent(X_train, y_train, 10000, 0.02, lambda_value)\n",
    "    y_hat = X_val.dot(w)\n",
    "\n",
    "    # get MSE for predictede w\n",
    "    val_mse = mse_function(y_hat, y_val)\n",
    "\n",
    "    if best_mse > val_mse:\n",
    "        best_mse = val_mse\n",
    "        best_value = lambda_value \n",
    "\n",
    "print(f'Best lambda value: {best_value} with mse: {best_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_prime_ridge(phi, w, y, n, lambda1):\n",
    "    return (2/n) * phi.T.dot(phi.dot(w) - y) + lambda1 * (2*w)\n",
    "\n",
    "def mse_function(y_pred, y):\n",
    "    return np.mean((y_pred- y)**2)\n",
    "\n",
    "def gradient_descent_ridge(phi, y, num_iterations, coef, lambda1):\n",
    "    n, m = phi.shape\n",
    "\n",
    "    w = np.random.rand(m, 1)  # Generate initial w\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        w -= coef * f_prime_ridge(phi, w, y, n, lambda1)\n",
    "\n",
    "    # MSE for final interation\n",
    "    final_mse = mse_function(phi.dot(w), y)\n",
    "\n",
    "    return w, final_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn mse for ridge: 0.03353159728443031\n",
      "Gradient descent mse for ridge: 13.093493548870391\n"
     ]
    }
   ],
   "source": [
    "# Find MSE for sklearn and gradient descent\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(X_train, y_train.flatten())\n",
    "y_hat = ridge.predict(X_train)\n",
    "y_hat = y_hat.reshape(-1, 1)\n",
    "\n",
    "sklearn_mse = mse_function(y_hat, y_train)\n",
    "print(f'sklearn mse for ridge: {sklearn_mse}')\n",
    "\n",
    "w, mse = gradient_descent_ridge(X_train, y_train, 10000, 0.0002, 1)\n",
    "print(f'Gradient descent mse for ridge: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: Best lambda value: 0.001291549665014884 with mse: 0.07015376052301812\n"
     ]
    }
   ],
   "source": [
    "# identify best lambda value\n",
    "values = np.logspace(-4, 1, 10) \n",
    "\n",
    "best_value = values[0] # initialize best value\n",
    "best_mse = float('inf') # Store intiial mse as very high value\n",
    "\n",
    "for lambda_value in values:\n",
    "    # Get predicted w for lambda\n",
    "    w, mse = gradient_descent_ridge(X_train, y_train, 10000, 0.02, lambda_value)\n",
    "    y_hat = X_val.dot(w)\n",
    "\n",
    "    # get MSE for predictede w\n",
    "    val_mse = mse_function(y_hat, y_val)\n",
    "\n",
    "    if best_mse > val_mse:\n",
    "        best_mse = val_mse\n",
    "        best_value = lambda_value \n",
    "\n",
    "print(f'Ridge: Best lambda value: {best_value} with mse: {best_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_prime_elasticnet(phi, w, y, n, lambda1, lambda2):\n",
    "    return (2/n) * phi.T.dot(phi.dot(w) - y) + lambda1*np.sign(w) + lambda2*(2*w)\n",
    "\n",
    "def mse_function(y_pred, y):\n",
    "    return np.mean((y_pred- y)**2)\n",
    "\n",
    "def gradient_descent_elasticnet(phi, y, num_iterations, coef, lambda1, lambda2):\n",
    "    n, m = phi.shape\n",
    "\n",
    "    w = np.random.rand(m, 1)  # Generate initial w\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        w -= coef * f_prime_elasticnet(phi, w, y, n, lambda1, lambda2)\n",
    "\n",
    "    # MSE for final interation\n",
    "    final_mse = mse_function(phi.dot(w), y)\n",
    "\n",
    "    return w, final_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn mse for Elastic Net: 23.66471886740332\n",
      "Gradient descent mse for Elastic Net: 18.556567442547877\n"
     ]
    }
   ],
   "source": [
    "# Find MSE for sklearn and gradient descent\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elasticnet = ElasticNet(alpha=2, l1_ratio=.5)\n",
    "elasticnet.fit(X_train, y_train.flatten())\n",
    "y_hat = elasticnet.predict(X_train)\n",
    "y_hat = y_hat.reshape(-1, 1)\n",
    "\n",
    "sklearn_mse = mse_function(y_hat, y_train)\n",
    "print(f'sklearn mse for Elastic Net: {sklearn_mse}')\n",
    "\n",
    "w, mse = gradient_descent_elasticnet(X_train, y_train, 10000, 0.0002, 1, 1)\n",
    "print(f'Gradient descent mse for Elastic Net: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net: Best lambda1 value: 0.0001 and best lambda2 value: 0.0001 with mse: 0.032383289169010836\n"
     ]
    }
   ],
   "source": [
    "values = np.logspace(-4, 1, 10) \n",
    "\n",
    "best_lambda1_value = values[0] # initialize best value\n",
    "best_lambda2_value = values[0]\n",
    "best_mse = float('inf') # Store intiial mse as very high value\n",
    "\n",
    "# Double iteration\n",
    "for lambda1_value in values:\n",
    "    for lambda2_value in values:\n",
    "        # Train on training data\n",
    "        w, mse = gradient_descent_elasticnet(X_train, y_train, 10000, 0.02, lambda1_value, lambda2_value)\n",
    "        \n",
    "        # get MSE for predicted w\n",
    "        y_hat = X_val.dot(w)\n",
    "        val_mse = mse_function(y_hat, y_val)\n",
    "\n",
    "        if best_mse > mse:\n",
    "            best_mse = mse\n",
    "            best_lambda1_value = lambda1_value\n",
    "            best_lambda2_value = lambda2_value \n",
    "\n",
    "print(f'Elastic Net: Best lambda1 value: {best_lambda1_value} and best lambda2 value: {best_lambda2_value} with mse: {best_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)\n",
    "Elastic net is better than using only one constraint because it has an mse of around 0.03, while lasso and ridge have an mse of .06 and .07 respectively"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
